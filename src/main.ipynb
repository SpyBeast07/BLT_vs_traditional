{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "853f066c",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6894c401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (10000, 2)\n",
      "Test shape: (2000, 2)\n",
      "\n",
      "Train head:\n",
      "                                               input  \\\n",
      "0               reconciliation trolls realized scene   \n",
      "1                        scratched kemp blah devices   \n",
      "2  delusional engineered perfect prey englishman ...   \n",
      "3  boomers nfl reacts parallels everything 6 redu...   \n",
      "4  patience put christmas superhero luc rake fulf...   \n",
      "\n",
      "                                              target  \n",
      "0               enecs dezilaer sllort noitailicnocer  \n",
      "1                        secived halb pmek dehctarcs  \n",
      "2  hctarcs detsub namhsilgne yerp tcefrep dereeni...  \n",
      "3  stcudnoc ysereh redlof secuder 6 gnihtyreve sl...  \n",
      "4  ylesned ylno elbats latnenitnoc dellifluf ekar...  \n",
      "\n",
      "Test head:\n",
      "                                               input  \\\n",
      "0  intimidated campaigns emerging marines spin be...   \n",
      "1            salary lebanese wifi fury fab sta polly   \n",
      "2  financing ahmed sexual cinematic puff malibu p...   \n",
      "3  n00 nickel disparity funded tutorials constrai...   \n",
      "4  unveiled alliance cleric skinned illness permi...   \n",
      "\n",
      "                                              target  \n",
      "0  gnisnaelc leb nips seniram gnigreme sngiapmac ...  \n",
      "1            yllop ats baf yruf ifiw esenabel yralas  \n",
      "2  hcaep ubilam ffup citamenic lauxes demha gnicn...  \n",
      "3  tseilrae tidercsid yllatigid ssaprus ycacovda ...  \n",
      "4  denoitats deyortsed noissimrep ssenlli denniks...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_path = \"./../data/train.csv\"\n",
    "test_path = \"./../data/test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "print(\"\\nTrain head:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest head:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51648900",
   "metadata": {},
   "source": [
    "# BLT Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a75530c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using MPS (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"‚úÖ Using MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"‚ö†Ô∏è MPS not available, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e996c3a4",
   "metadata": {},
   "source": [
    "## Patcher (entropy-based segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb0d88",
   "metadata": {},
   "source": [
    "**Shannon Entropy Function**\n",
    "\n",
    "Helper to compute entropy of a sequence of characters.\n",
    "\n",
    "H = - \\sum p(x) \\cdot \\log_2(p(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82277daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def shannon_entropy(text: str) -> float:\n",
    "    \"\"\"Compute Shannon entropy of a string.\"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    counts = Counter(text)\n",
    "    probs = [count / len(text) for count in counts.values()]\n",
    "    return -sum(p * math.log2(p) for p in probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0b387",
   "metadata": {},
   "source": [
    "**Patcher Function**\n",
    "\n",
    "- Use sliding window of size W=10.\n",
    "- Keep adding characters to current patch until either:\n",
    "    - Entropy > threshold(2.0)\n",
    "    - Patch length > 15\n",
    "- Then start a new patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38e3a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(text: str, window_size=10, entropy_threshold=2.0, max_patch_len=15):\n",
    "    patches = []\n",
    "    current_patch = \"\"\n",
    "\n",
    "    for ch in text:\n",
    "        current_patch += ch\n",
    "\n",
    "        # Compute entropy only when window_size reached\n",
    "        entropy = (\n",
    "            shannon_entropy(current_patch[-window_size:])\n",
    "            if len(current_patch) >= window_size else 0\n",
    "        )\n",
    "\n",
    "        # Split condition: high entropy OR too long\n",
    "        if entropy > entropy_threshold or len(current_patch) >= max_patch_len:\n",
    "            if current_patch.strip():\n",
    "                patches.append(current_patch.strip())\n",
    "            current_patch = \"\"\n",
    "\n",
    "    # Add leftover patch\n",
    "    if current_patch.strip():\n",
    "        patches.append(current_patch.strip())\n",
    "\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14303cb",
   "metadata": {},
   "source": [
    "**Test Patcher**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "957e4446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: reconciliation trolls realized scene\n",
      "Patches: ['reconcilia', 'tion troll', 's realized', 'scene']\n",
      "\n",
      "Text: LMA is fun!\n",
      "Patches: ['LMA is fun', '!']\n",
      "\n",
      "Text: aaaaabbbbbcccccddddd\n",
      "Patches: ['aaaaabbbbbccccc', 'ddddd']\n"
     ]
    }
   ],
   "source": [
    "sample_texts = [\n",
    "    \"reconciliation trolls realized scene\", # High entropy, more splits\n",
    "    \"LMA is fun!\", # Based on threshold, may not split\n",
    "    \"aaaaabbbbbcccccddddd\"  # low entropy predictable\n",
    "]\n",
    "\n",
    "for txt in sample_texts:\n",
    "    patches = patchify(txt)\n",
    "    print(f\"\\nText: {txt}\")\n",
    "    print(\"Patches:\", patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6433b9b",
   "metadata": {},
   "source": [
    "## Hash N-Gram Embeddings\n",
    "\n",
    "1. Extract all n-grams (n=1,2,3) from each patch.\n",
    "2. Map each n-gram into a bucket in [0, 4095].\n",
    "3. Use an embedding lookup table (nn.Embedding) to get a 64-d vector.\n",
    "4. Sum all vectors ‚Üí final patch embedding (shape = [64])."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b7888f",
   "metadata": {},
   "source": [
    "**Hash Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "934382cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def hash_ngram(ngram: str, num_buckets=4096) -> int:\n",
    "    \"\"\"Hash an n-gram string into a bucket [0, num_buckets-1].\"\"\"\n",
    "    return int(hashlib.md5(ngram.encode(\"utf-8\")).hexdigest(), 16) % num_buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1af6fd",
   "metadata": {},
   "source": [
    "**N-Gram Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d4f8101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(text: str, n: int):\n",
    "    return [text[i:i+n] for i in range(len(text)-n+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844bc8a1",
   "metadata": {},
   "source": [
    "**Patch Embedding Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eaee92ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchEmbedder(nn.Module):\n",
    "    def __init__(self, num_buckets=4096, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            \"1\": nn.Embedding(num_buckets, embed_dim),\n",
    "            \"2\": nn.Embedding(num_buckets, embed_dim),\n",
    "            \"3\": nn.Embedding(num_buckets, embed_dim),\n",
    "        })\n",
    "        # Xavier init\n",
    "        for emb in self.embeddings.values():\n",
    "            nn.init.xavier_uniform_(emb.weight)\n",
    "\n",
    "        self.num_buckets = num_buckets\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, patch: str):\n",
    "        \"\"\"Convert one patch string into a [embed_dim] vector.\"\"\"\n",
    "        vectors = []\n",
    "        # get device from embedding params\n",
    "        device = next(self.embeddings[\"1\"].parameters()).device  \n",
    "\n",
    "        for n in [1, 2, 3]:\n",
    "            ngrams = extract_ngrams(patch, n)\n",
    "            for ng in ngrams:\n",
    "                bucket = hash_ngram(ng, self.num_buckets)\n",
    "                idx = torch.tensor(bucket, dtype=torch.long, device=device)\n",
    "                vectors.append(self.embeddings[str(n)](idx))\n",
    "\n",
    "        if len(vectors) == 0:\n",
    "            return torch.zeros(self.embed_dim, device=device)\n",
    "\n",
    "        return torch.stack(vectors, dim=0).sum(dim=0)  # sum across n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350e4c9",
   "metadata": {},
   "source": [
    "**Test It on Sample Patches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "61d18f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Patch: reconcilia | Embedding shape: torch.Size([64])\n",
      "Patch: tion troll | Embedding shape: torch.Size([64])\n",
      "Patch: s realized | Embedding shape: torch.Size([64])\n",
      "Patch:  scene | Embedding shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "embedder = PatchEmbedder().to(device)\n",
    "\n",
    "sample_patches = [\"reconcilia\", \"tion troll\", \"s realized\", \" scene\"]\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "for patch in sample_patches:\n",
    "    vec = embedder(patch)\n",
    "    print(f\"Patch: {patch} | Embedding shape: {vec.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104ee9f",
   "metadata": {},
   "source": [
    "## BLT Dataset Class\n",
    "\n",
    "This dataset will:\n",
    "1. Read train.csv / test.csv.\n",
    "2. For each row:\n",
    "    - Take input string ‚Üí apply patchify ‚Üí embed patches into [seq_len, 64].\n",
    "    - Take target string ‚Üí here we‚Äôll keep it character-level for decoder supervision (simpler than patching the output).\n",
    "3. Return tensors for (src_seq, tgt_seq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6c261c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class BLTDataset(Dataset):\n",
    "    def __init__(self, csv_path, patch_embedder, \n",
    "                    window_size=10, entropy_threshold=2.0, max_patch_len=15):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.patch_embedder = patch_embedder\n",
    "        self.window_size = window_size\n",
    "        self.entropy_threshold = entropy_threshold\n",
    "        self.max_patch_len = max_patch_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        inp, tgt = row[\"input\"], row[\"target\"]\n",
    "\n",
    "        # --- Input sequence: patch embeddings ---\n",
    "        patches = patchify(inp, \n",
    "                            window_size=self.window_size,\n",
    "                            entropy_threshold=self.entropy_threshold,\n",
    "                            max_patch_len=self.max_patch_len)\n",
    "        patch_vecs = [self.patch_embedder(p) for p in patches]\n",
    "        if len(patch_vecs) == 0:\n",
    "            src_seq = torch.zeros((1, self.patch_embedder.embed_dim))\n",
    "        else:\n",
    "            src_seq = torch.stack(patch_vecs, dim=0)  # [num_patches, 64]\n",
    "\n",
    "        # --- Target sequence: character-level (convert chars to IDs) ---\n",
    "        tgt_ids = torch.tensor(list(tgt.encode(\"utf-8\")), dtype=torch.long)\n",
    "        # NOTE: ord(c) = ASCII code (works since only printable ASCII)\n",
    "\n",
    "        return src_seq, tgt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "daa5889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (patch embeddings): torch.Size([4, 64])\n",
      "Target shape (char ids): torch.Size([36])\n",
      "Target IDs: tensor([101, 110, 101,  99, 115,  32, 100, 101, 122, 105])\n",
      "Target string (reconstructed): enecs dezilaer sllort noitailicnocer\n"
     ]
    }
   ],
   "source": [
    "train_ds = BLTDataset(\"./../data/train.csv\", patch_embedder=embedder)\n",
    "test_ds = BLTDataset(\"./../data/test.csv\", patch_embedder=embedder)\n",
    "\n",
    "# Get first sample\n",
    "src_seq, tgt_ids = train_ds[0]\n",
    "\n",
    "print(\"Input shape (patch embeddings):\", src_seq.shape)  # [num_patches, 64]\n",
    "print(\"Target shape (char ids):\", tgt_ids.shape)          # [target_len]\n",
    "print(\"Target IDs:\", tgt_ids[:10])\n",
    "print(\"Target string (reconstructed):\", \"\".join([chr(x) for x in tgt_ids]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780e7a1",
   "metadata": {},
   "source": [
    "## Collate Function for BLT\n",
    "\n",
    "We‚Äôll:\n",
    "1. Take a batch of (src_seq, tgt_seq).\n",
    "2. Pad src_seq to [batch, max_src_len, 64].\n",
    "3. Pad tgt_seq to [batch, max_tgt_len].\n",
    "4. Return padded tensors + lengths (useful for masking in the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89b5f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def blt_collate_fn(batch, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    batch: list of (src_seq, tgt_seq) from BLTDataset\n",
    "    \"\"\"\n",
    "    src_seqs, tgt_seqs = zip(*batch)  # unpack\n",
    "\n",
    "    # --- Pad source (patch embeddings) ---\n",
    "    # src_seqs is list of [num_patches, 64]\n",
    "    # pad_sequence requires same shape in last dim\n",
    "    src_padded = pad_sequence(src_seqs, batch_first=True, padding_value=0.0)\n",
    "    # Shape: [B, max_src_len, 64]\n",
    "\n",
    "    src_lengths = torch.tensor([s.size(0) for s in src_seqs], dtype=torch.long)\n",
    "\n",
    "    # --- Pad target (char IDs) ---\n",
    "    tgt_padded = pad_sequence(tgt_seqs, batch_first=True, padding_value=0)\n",
    "    # Shape: [B, max_tgt_len]\n",
    "\n",
    "    tgt_lengths = torch.tensor([t.size(0) for t in tgt_seqs], dtype=torch.long)\n",
    "\n",
    "    return (src_padded.to(device),\n",
    "            src_lengths.to(device),\n",
    "            tgt_padded.to(device),\n",
    "            tgt_lengths.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "47d6674f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source batch shape: torch.Size([8, 14, 64])\n",
      "Source lengths: tensor([10,  7,  4,  6, 14, 14,  9,  7], device='mps:0')\n",
      "Target batch shape: torch.Size([8, 135])\n",
      "Target lengths: tensor([ 93,  62,  31,  51, 135, 134,  87,  65], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.9 * len(train_ds))\n",
    "val_size   = len(train_ds) - train_size\n",
    "train_subset, val_subset = random_split(train_ds, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda b: blt_collate_fn(b, device=device)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_subset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: blt_collate_fn(b, device=device)\n",
    ")\n",
    "\n",
    "# Fetch a batch\n",
    "src_padded, src_lengths, tgt_padded, tgt_lengths = next(iter(train_loader))\n",
    "\n",
    "print(\"Source batch shape:\", src_padded.shape)   # [B, max_src_len, 64]\n",
    "print(\"Source lengths:\", src_lengths)\n",
    "print(\"Target batch shape:\", tgt_padded.shape)   # [B, max_tgt_len]\n",
    "print(\"Target lengths:\", tgt_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b0951",
   "metadata": {},
   "source": [
    "## BLT Model Architecture\n",
    "\n",
    "Core Idea:\n",
    "1. Encoder: Take patch embeddings [B, L, 64], map into hidden dimension with a transformer encoder.\n",
    "2. Decoder: Generate characters (IDs) step by step using a transformer decoder.\n",
    "3. Output layer: Linear projection ‚Üí vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f28faa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BLTModel(nn.Module):\n",
    "    def __init__(self, embed_dim=64, hidden_dim=128, vocab_size=256, num_layers=2, nhead=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # project patch embeddings to hidden size\n",
    "        self.input_proj = nn.Linear(embed_dim, hidden_dim)\n",
    "\n",
    "        # Positional encoding for encoder & decoder\n",
    "        self.pos_encoder = nn.Embedding(512, hidden_dim)  # max 512 patches\n",
    "        self.pos_decoder = nn.Embedding(512, hidden_dim)  # max 512 chars\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers, enable_nested_tensor=False)\n",
    "\n",
    "        # Transformer decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=nhead, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Char embedding for targets\n",
    "        self.tgt_embed = nn.Embedding(vocab_size, hidden_dim)\n",
    "\n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, src_lengths, tgt_inp):\n",
    "        \"\"\"\n",
    "        src: [B, Ls, 64]    (patch embeddings)\n",
    "        src_lengths: [B]    (not used yet, but can mask)\n",
    "        tgt_inp: [B, Lt]    (char ids, input shifted right)\n",
    "        \"\"\"\n",
    "\n",
    "        B, Ls, _ = src.shape\n",
    "        B, Lt = tgt_inp.shape\n",
    "\n",
    "        # --- Encoder ---\n",
    "        src_emb = self.input_proj(src)  # [B, Ls, H]\n",
    "        pos_src = self.pos_encoder(torch.arange(Ls, device=src.device)).unsqueeze(0)  # [1, Ls, H]\n",
    "        src_key_padding_mask = (src.sum(dim=-1) == 0)  # [B, Ls], True for padding\n",
    "        memory = self.encoder(src_emb, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        # --- Decoder ---\n",
    "        tgt_emb = self.tgt_embed(tgt_inp)  # [B, Lt, H]\n",
    "        pos_tgt = self.pos_decoder(torch.arange(Lt, device=tgt_inp.device)).unsqueeze(0)  # [1, Lt, H]\n",
    "        tgt_emb = tgt_emb + pos_tgt\n",
    "\n",
    "        # causal mask for autoregressive decoding\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(Lt).to(tgt_inp.device)\n",
    "\n",
    "        tgt_key_padding_mask = (tgt_inp == 0)  # [B, Lt]\n",
    "        out = self.decoder(\n",
    "            tgt=tgt_emb,\n",
    "            memory=memory,\n",
    "            tgt_mask=causal_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "\n",
    "        logits = self.output_proj(out)  # [B, Lt, vocab_size]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e965f57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([8, 105, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/functional.py:6041: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hyperparams\n",
    "VOCAB_SIZE = 256  # byte-level\n",
    "model = BLTModel(embed_dim=64, hidden_dim=128, vocab_size=VOCAB_SIZE).to(device)\n",
    "\n",
    "# Dummy batch\n",
    "src_padded, src_lengths, tgt_padded, tgt_lengths = next(iter(train_loader))\n",
    "\n",
    "# Teacher forcing: shift target for decoder input\n",
    "tgt_inp = tgt_padded[:, :-1]   # input\n",
    "tgt_out = tgt_padded[:, 1:]    # expected output\n",
    "\n",
    "logits = model(src_padded, src_lengths, tgt_inp)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)  # [B, Lt-1, vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25767bd",
   "metadata": {},
   "source": [
    "## Training Loop + Checkpoints\n",
    "\n",
    "We‚Äôll set up:\n",
    "1. Loss = CrossEntropyLoss(ignore_index=0) (ignores PAD tokens).\n",
    "2. Optimizer = Adam.\n",
    "3. Training loop with logging.\n",
    "4. Checkpoint saving (state_dict, optimizer, epoch, loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "361dc301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# ----------------------------\n",
    "# Training loop for BLT model\n",
    "# ----------------------------\n",
    "def train_blt(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader=None, \n",
    "    num_epochs=5, \n",
    "    lr=1e-3, \n",
    "    device=None, \n",
    "    save_every=10, \n",
    "    resume_path=None\n",
    "):\n",
    "    if device is None:\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "    print(f\"Training on: {device}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"min\",      # minimize validation loss\n",
    "        factor=0.5,      # reduce LR by half\n",
    "        patience=5       # wait 5 epochs without improvement\n",
    "    )\n",
    "\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "    # Resume\n",
    "    start_epoch = 1\n",
    "    if resume_path and os.path.exists(resume_path):\n",
    "        print(f\"üîÑ Resuming from checkpoint: {resume_path}\")\n",
    "        checkpoint = torch.load(resume_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        if \"scheduler_state\" in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
    "        print(f\"‚úÖ Resumed from epoch {checkpoint['epoch']} (loss {checkpoint['loss']:.4f})\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            src, src_lengths, tgt, tgt_lengths = batch\n",
    "            src, src_lengths = src.to(device), src_lengths.to(device)\n",
    "            tgt, tgt_lengths = tgt.to(device), tgt_lengths.to(device)\n",
    "\n",
    "            tgt_inp = tgt[:, :-1]\n",
    "            tgt_out = tgt[:, 1:]\n",
    "\n",
    "            logits = model(src, src_lengths, tgt_inp)\n",
    "\n",
    "            loss = criterion(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                tgt_out.reshape(-1)\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * tgt_out.numel()\n",
    "            total_tokens += tgt_out.numel()\n",
    "\n",
    "        avg_loss = total_loss / total_tokens\n",
    "        print(f\"\\nüìò Epoch {epoch}/{num_epochs} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if val_loader is not None:\n",
    "            val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "            perplexity = math.exp(val_loss)\n",
    "            print(f\"üìó Validation - Loss: {val_loss:.4f}, Perplexity: {perplexity:.2f}, Token Acc: {val_acc:.2f}%\")\n",
    "\n",
    "            old_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            scheduler.step(val_loss)\n",
    "            new_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            if new_lr != old_lr:\n",
    "                print(f\"üîΩ LR reduced: {old_lr:.6f} ‚Üí {new_lr:.6f}\")\n",
    "\n",
    "        if epoch % save_every == 0 or epoch == num_epochs:\n",
    "            ckpt_path = f\"checkpoints/blt_epoch{epoch}.pt\"\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"scheduler_state\": scheduler.state_dict(),\n",
    "                \"loss\": avg_loss\n",
    "            }, ckpt_path)\n",
    "            print(f\"‚úÖ Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "\n",
    "    for batch in val_loader:\n",
    "        src, src_lengths, tgt, tgt_lengths = batch\n",
    "        src, src_lengths = src.to(device), src_lengths.to(device)\n",
    "        tgt, tgt_lengths = tgt.to(device), tgt_lengths.to(device)\n",
    "\n",
    "        tgt_inp = tgt[:, :-1]\n",
    "        tgt_out = tgt[:, 1:]\n",
    "\n",
    "        logits = model(src, src_lengths, tgt_inp)\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            tgt_out.reshape(-1)\n",
    "        )\n",
    "        total_loss += loss.item() * tgt_out.numel()\n",
    "        total_tokens += tgt_out.numel()\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        mask = tgt_out != 0\n",
    "        correct_tokens += ((preds == tgt_out) & mask).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    accuracy = 100.0 * correct_tokens / total_tokens\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b94386c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: mps\n",
      "üîÑ Resuming from checkpoint: checkpoints/blt_epoch80.pt\n",
      "‚úÖ Resumed from epoch 80 (loss 1.5632)\n",
      "\n",
      "üìò Epoch 81/1000 - Train Loss: 1.5370\n",
      "üìó Validation - Loss: 1.3799, Perplexity: 3.97, Token Acc: 38.56%\n",
      "\n",
      "üìò Epoch 82/1000 - Train Loss: 1.5328\n",
      "üìó Validation - Loss: 1.3862, Perplexity: 4.00, Token Acc: 38.45%\n",
      "‚úÖ Saved checkpoint: checkpoints/blt_epoch82.pt\n",
      "\n",
      "üìò Epoch 83/1000 - Train Loss: 1.5338\n",
      "üìó Validation - Loss: 1.3877, Perplexity: 4.01, Token Acc: 38.39%\n",
      "\n",
      "üìò Epoch 84/1000 - Train Loss: 1.5309\n",
      "üìó Validation - Loss: 1.3899, Perplexity: 4.01, Token Acc: 38.29%\n",
      "‚úÖ Saved checkpoint: checkpoints/blt_epoch84.pt\n",
      "\n",
      "üìò Epoch 85/1000 - Train Loss: 1.5299\n",
      "üìó Validation - Loss: 1.3910, Perplexity: 4.02, Token Acc: 38.29%\n",
      "\n",
      "üìò Epoch 86/1000 - Train Loss: 1.5282\n",
      "üìó Validation - Loss: 1.3953, Perplexity: 4.04, Token Acc: 38.18%\n",
      "‚úÖ Saved checkpoint: checkpoints/blt_epoch86.pt\n",
      "\n",
      "üìò Epoch 87/1000 - Train Loss: 1.5250\n",
      "üìó Validation - Loss: 1.3927, Perplexity: 4.03, Token Acc: 38.18%\n",
      "üîΩ LR reduced: 0.001000 ‚Üí 0.000500\n",
      "\n",
      "üìò Epoch 88/1000 - Train Loss: 1.4990\n",
      "üìó Validation - Loss: 1.3806, Perplexity: 3.98, Token Acc: 38.41%\n",
      "‚úÖ Saved checkpoint: checkpoints/blt_epoch88.pt\n",
      "\n",
      "üìò Epoch 89/1000 - Train Loss: 1.4912\n",
      "üìó Validation - Loss: 1.3785, Perplexity: 3.97, Token Acc: 38.51%\n",
      "\n",
      "üìò Epoch 90/1000 - Train Loss: 1.4891\n",
      "üìó Validation - Loss: 1.3778, Perplexity: 3.97, Token Acc: 38.50%\n",
      "‚úÖ Saved checkpoint: checkpoints/blt_epoch90.pt\n",
      "\n",
      "üìò Epoch 91/1000 - Train Loss: 1.4859\n",
      "üìó Validation - Loss: 1.3763, Perplexity: 3.96, Token Acc: 38.52%\n",
      "\n",
      "üìò Epoch 92/1000 - Train Loss: 1.4855\n",
      "üìó Validation - Loss: 1.3757, Perplexity: 3.96, Token Acc: 38.50%\n",
      "‚úÖ Saved checkpoint: checkpoints/blt_epoch92.pt\n",
      "\n",
      "üìò Epoch 93/1000 - Train Loss: 1.4840\n",
      "üìó Validation - Loss: 1.3769, Perplexity: 3.96, Token Acc: 38.50%\n",
      "\n",
      "üìò Epoch 94/1000 - Train Loss: 1.4820\n",
      "üìó Validation - Loss: 1.3755, Perplexity: 3.96, Token Acc: 38.59%\n",
      "‚úÖ Saved checkpoint: checkpoints/blt_epoch94.pt\n",
      "\n",
      "üìò Epoch 95/1000 - Train Loss: 1.4814\n",
      "üìó Validation - Loss: 1.3764, Perplexity: 3.96, Token Acc: 38.55%\n",
      "\n",
      "üìò Epoch 96/1000 - Train Loss: 1.4800\n",
      "üìó Validation - Loss: 1.3758, Perplexity: 3.96, Token Acc: 38.53%\n",
      "‚úÖ Saved checkpoint: checkpoints/blt_epoch96.pt\n",
      "\n",
      "üìò Epoch 97/1000 - Train Loss: 1.4802\n",
      "üìó Validation - Loss: 1.3757, Perplexity: 3.96, Token Acc: 38.47%\n",
      "\n",
      "üìò Epoch 98/1000 - Train Loss: 1.4785\n",
      "üìó Validation - Loss: 1.3764, Perplexity: 3.96, Token Acc: 38.53%\n",
      "‚úÖ Saved checkpoint: checkpoints/blt_epoch98.pt\n",
      "\n",
      "üìò Epoch 99/1000 - Train Loss: 1.4785\n",
      "üìó Validation - Loss: 1.3756, Perplexity: 3.96, Token Acc: 38.54%\n",
      "\n",
      "üìò Epoch 100/1000 - Train Loss: 1.4767\n",
      "üìó Validation - Loss: 1.3744, Perplexity: 3.95, Token Acc: 38.56%\n",
      "‚úÖ Saved checkpoint: checkpoints/blt_epoch100.pt\n",
      "\n",
      "üìò Epoch 101/1000 - Train Loss: 1.4749\n",
      "üìó Validation - Loss: 1.3754, Perplexity: 3.96, Token Acc: 38.54%\n",
      "\n",
      "üìò Epoch 102/1000 - Train Loss: 1.4759\n",
      "üìó Validation - Loss: 1.3737, Perplexity: 3.95, Token Acc: 38.59%\n",
      "‚úÖ Saved checkpoint: checkpoints/blt_epoch102.pt\n",
      "\n",
      "üìò Epoch 103/1000 - Train Loss: 1.4741\n",
      "üìó Validation - Loss: 1.3757, Perplexity: 3.96, Token Acc: 38.49%\n",
      "\n",
      "üìò Epoch 104/1000 - Train Loss: 1.4753\n",
      "üìó Validation - Loss: 1.3747, Perplexity: 3.95, Token Acc: 38.53%\n",
      "‚úÖ Saved checkpoint: checkpoints/blt_epoch104.pt\n",
      "\n",
      "üìò Epoch 105/1000 - Train Loss: 1.4724\n",
      "üìó Validation - Loss: 1.3732, Perplexity: 3.95, Token Acc: 38.50%\n",
      "\n",
      "üìò Epoch 106/1000 - Train Loss: 1.4724\n",
      "üìó Validation - Loss: 1.3729, Perplexity: 3.95, Token Acc: 38.54%\n",
      "‚úÖ Saved checkpoint: checkpoints/blt_epoch106.pt\n",
      "\n",
      "üìò Epoch 107/1000 - Train Loss: 1.4711\n",
      "üìó Validation - Loss: 1.3748, Perplexity: 3.95, Token Acc: 38.51%\n",
      "\n",
      "üìò Epoch 108/1000 - Train Loss: 1.4719\n",
      "üìó Validation - Loss: 1.3732, Perplexity: 3.95, Token Acc: 38.58%\n",
      "‚úÖ Saved checkpoint: checkpoints/blt_epoch108.pt\n",
      "\n",
      "üìò Epoch 109/1000 - Train Loss: 1.4705\n",
      "üìó Validation - Loss: 1.3751, Perplexity: 3.96, Token Acc: 38.51%\n",
      "\n",
      "üìò Epoch 110/1000 - Train Loss: 1.4687\n",
      "üìó Validation - Loss: 1.3736, Perplexity: 3.95, Token Acc: 38.58%\n",
      "‚úÖ Saved checkpoint: checkpoints/blt_epoch110.pt\n",
      "\n",
      "üìò Epoch 111/1000 - Train Loss: 1.4696\n",
      "üìó Validation - Loss: 1.3728, Perplexity: 3.95, Token Acc: 38.54%\n",
      "\n",
      "üìò Epoch 112/1000 - Train Loss: 1.4690\n",
      "üìó Validation - Loss: 1.3742, Perplexity: 3.95, Token Acc: 38.49%\n",
      "üîΩ LR reduced: 0.000500 ‚Üí 0.000250\n",
      "‚úÖ Saved checkpoint: checkpoints/blt_epoch112.pt\n"
     ]
    }
   ],
   "source": [
    "train_blt(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader=val_loader,\n",
    "    num_epochs=1000, \n",
    "    save_every=2, \n",
    "    resume_path=\"checkpoints/blt_epoch80.pt\", \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3cfa26",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b1896e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on: cpu\n",
      "‚úÖ Loaded checkpoint: checkpoints/blt_epoch80.pt (epoch 80, loss 1.5632)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/functional.py:6041: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Test Results - Loss: 1.4813, Token Accuracy: 53.33%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "# --- Setup device ---\n",
    "# ----------------------------\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Testing on: {device}\")\n",
    "\n",
    "# ----------------------------\n",
    "# --- Load patch embedder ---\n",
    "# ----------------------------\n",
    "# Assuming you already have PatchEmbedder class\n",
    "embedder = PatchEmbedder(num_buckets=4096, embed_dim=64)\n",
    "\n",
    "# ----------------------------\n",
    "# --- Load test dataset ---\n",
    "# ----------------------------\n",
    "test_ds = BLTDataset(\"./../data/test.csv\", patch_embedder=embedder)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: blt_collate_fn(b, device=device)\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# --- Load model checkpoint ---\n",
    "# ----------------------------\n",
    "VOCAB_SIZE = 256\n",
    "model = BLTModel(embed_dim=64, hidden_dim=128, vocab_size=VOCAB_SIZE).to(device)\n",
    "\n",
    "ckpt_path = \"checkpoints/blt_epoch80.pt\"\n",
    "if not os.path.exists(ckpt_path):\n",
    "    raise FileNotFoundError(f\"{ckpt_path} not found!\")\n",
    "\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model.eval()\n",
    "print(f\"‚úÖ Loaded checkpoint: {ckpt_path} (epoch {checkpoint['epoch']}, loss {checkpoint['loss']:.4f})\")\n",
    "\n",
    "# ----------------------------\n",
    "# --- Testing loop ---\n",
    "# ----------------------------\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "total_loss = 0.0\n",
    "total_tokens = 0\n",
    "correct_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        src, src_lengths, tgt, tgt_lengths = batch\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        tgt_inp = tgt[:, :-1]\n",
    "        tgt_out = tgt[:, 1:]\n",
    "\n",
    "        logits = model(src, src_lengths, tgt_inp)\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Accuracy\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        mask = tgt_out != 0\n",
    "        correct_tokens += ((preds == tgt_out) & mask).sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "avg_loss = total_loss / len(test_loader)\n",
    "accuracy = 100.0 * correct_tokens / total_tokens\n",
    "\n",
    "print(f\"\\nüß™ Test Results - Loss: {avg_loss:.4f}, Token Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc2f078",
   "metadata": {},
   "source": [
    "## Validate BLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acfd61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def validate_blt(model, test_loader, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            src, src_lengths, tgt, tgt_lengths = batch\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            tgt_inp = tgt[:, :-1]\n",
    "            tgt_out = tgt[:, 1:]\n",
    "\n",
    "            logits = model(src, src_lengths, tgt_inp)  # [B, Lt-1, vocab]\n",
    "            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "\n",
    "            total_loss += loss.item() * tgt_out.numel()  # sum over tokens\n",
    "            total_tokens += tgt_out.numel()\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    print(f\"‚úÖ Test Loss: {avg_loss:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479c821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test Loss: 1.4787, Perplexity: 4.3871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.4786781151334818, tensor(4.3871))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: blt_collate_fn(b, device=device)\n",
    ")\n",
    "\n",
    "validate_blt(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fb7ca9",
   "metadata": {},
   "source": [
    "## Inference Function (Text-to-Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b4d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_blt(model, input_text, patch_embedder, max_len=256, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # --- Prepare encoder input ---\n",
    "    patches = patchify(input_text)\n",
    "    src_seq = torch.stack([patch_embedder(p) for p in patches], dim=0).unsqueeze(0).to(device)  # [1, Ls, H]\n",
    "    src_lengths = torch.tensor([src_seq.size(1)], dtype=torch.long).to(device)\n",
    "\n",
    "    # --- Encoder memory ---\n",
    "    memory = model.encoder(model.input_proj(src_seq) + model.pos_encoder(torch.arange(src_seq.size(1), device=device)).unsqueeze(0))\n",
    "\n",
    "    # --- Autoregressive decoding ---\n",
    "    output_ids = [0]  # start with PAD / or custom start token if you have one\n",
    "    for t in range(max_len):\n",
    "        tgt_inp = torch.tensor(output_ids, dtype=torch.long, device=device).unsqueeze(0)  # [1, t+1]\n",
    "        tgt_emb = model.tgt_embed(tgt_inp) + model.pos_decoder(torch.arange(t+1, device=device)).unsqueeze(0)\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(t+1).to(device)\n",
    "\n",
    "        out = model.decoder(tgt_emb, memory, tgt_mask=causal_mask)\n",
    "        logits = model.output_proj(out)  # [1, t+1, vocab]\n",
    "        next_id = logits[0, -1].argmax().item()  # greedy decoding\n",
    "        output_ids.append(next_id)\n",
    "\n",
    "        if next_id == 0:  # stop at PAD (or EOS token)\n",
    "            break\n",
    "\n",
    "    # Convert IDs to string\n",
    "    output_text = \"\".join([chr(i) for i in output_ids[1:]])  # skip first PAD/start token\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cd3281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : reconciliation trolls realized scene\n",
      "Output: airetsyh seitilibapac seiranidro seiraniretsym seitilibapac seitilibapac seitilibapac seitilibapac s\n"
     ]
    }
   ],
   "source": [
    "input_text = \"reconciliation trolls realized scene\"\n",
    "output_text = generate_blt(model, input_text, patch_embedder=embedder, max_len=100, device=device)\n",
    "print(\"Input :\", input_text)\n",
    "print(\"Output:\", output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebaf1f",
   "metadata": {},
   "source": [
    "## Prediction on test data and saving CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc3475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- Make sure device is set ---\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"üîç Using device: {device}\")\n",
    "\n",
    "# --- Create prediction folder ---\n",
    "os.makedirs(\"prediction\", exist_ok=True)\n",
    "\n",
    "# --- Load test dataset ---\n",
    "test_df = pd.read_csv(\"./../data/test.csv\")\n",
    "\n",
    "# --- Simple Dataset wrapper ---\n",
    "class SimpleTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df[\"input\"].tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx]\n",
    "\n",
    "test_ds = SimpleTestDataset(test_df)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,  # autoregressive, keep 1\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# --- Function to generate single prediction ---\n",
    "def generate_single(input_text, model, patch_embedder, max_len=256):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        patches = patchify(input_text)\n",
    "        if len(patches) == 0:\n",
    "            return \"\"  # handle empty input\n",
    "\n",
    "        # Encode source sequence\n",
    "        src_seq = torch.stack([patch_embedder(p) for p in patches], dim=0).unsqueeze(0).to(device)\n",
    "        src_lengths = torch.tensor([src_seq.size(1)], dtype=torch.long).to(device)\n",
    "\n",
    "        src_emb = model.input_proj(src_seq)\n",
    "        pos_src = model.pos_encoder(torch.arange(src_seq.size(1), device=device)).unsqueeze(0)\n",
    "        memory = model.encoder(src_emb + pos_src)\n",
    "\n",
    "        # Autoregressive decoding (greedy)\n",
    "        output_ids = [ord(\" \")]  # crude <SOS>: space token\n",
    "        for t in range(max_len):\n",
    "            tgt_inp = torch.tensor(output_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            pos_tgt = model.pos_decoder(torch.arange(t+1, device=device)).unsqueeze(0)\n",
    "            tgt_emb = model.tgt_embed(tgt_inp) + pos_tgt\n",
    "            causal_mask = nn.Transformer.generate_square_subsequent_mask(t+1).to(device)\n",
    "\n",
    "            out = model.decoder(tgt=tgt_emb, memory=memory, tgt_mask=causal_mask)\n",
    "            logits = model.output_proj(out)  # [1, t+1, vocab]\n",
    "\n",
    "            next_id = logits[0, -1].argmax().item()\n",
    "            output_ids.append(next_id)\n",
    "\n",
    "            # stop if <pad> (0) or newline (10) appears\n",
    "            if next_id in [0, 10]:\n",
    "                break\n",
    "\n",
    "        pred_text = \"\".join([chr(i) for i in output_ids[1:]])  # skip fake SOS\n",
    "        return pred_text\n",
    "\n",
    "# --- Generate predictions ---\n",
    "model.to(device)\n",
    "model.eval()\n",
    "predictions = []\n",
    "for input_text in test_loader:\n",
    "    input_str = input_text[0]  # batch size 1\n",
    "    pred = generate_single(input_str, model, patch_embedder=embedder)\n",
    "    predictions.append(pred)\n",
    "\n",
    "# --- Save predictions ---\n",
    "test_df[\"prediction\"] = predictions\n",
    "output_path = \"./prediction/blt_predictions.csv\"\n",
    "test_df.to_csv(output_path, index=False)\n",
    "print(f\"‚úÖ Predictions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e59d25",
   "metadata": {},
   "source": [
    "# Character level Model & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d808956",
   "metadata": {},
   "source": [
    "## Data & Tokenizer\n",
    "\n",
    "1. Loads your train.csv and test.csv.\n",
    "2. Creates a character vocabulary:\n",
    "    - [PAD], [SOS], [EOS] + printable ASCII chars.\n",
    "    - Maps chars ‚Üî IDs (stoi, itos).\n",
    "3. Provides encode/decode functions.\n",
    "4. Tests it on a sample string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbffc4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (10000, 2)\n",
      "Test shape: (2000, 2)\n",
      "                                               input  \\\n",
      "0               reconciliation trolls realized scene   \n",
      "1                        scratched kemp blah devices   \n",
      "2  delusional engineered perfect prey englishman ...   \n",
      "3  boomers nfl reacts parallels everything 6 redu...   \n",
      "4  patience put christmas superhero luc rake fulf...   \n",
      "\n",
      "                                              target  \n",
      "0               enecs dezilaer sllort noitailicnocer  \n",
      "1                        secived halb pmek dehctarcs  \n",
      "2  hctarcs detsub namhsilgne yerp tcefrep dereeni...  \n",
      "3  stcudnoc ysereh redlof secuder 6 gnihtyreve sl...  \n",
      "4  ylesned ylno elbats latnenitnoc dellifluf ekar...  \n",
      "\n",
      "Vocab size: 98\n",
      "PAD idx: 0 SOS idx: 1 EOS idx: 2\n",
      "Sample: LMA is fun!\n",
      "Encoded: [1, 47, 48, 36, 3, 76, 86, 3, 73, 88, 81, 4, 2]\n",
      "Decoded: LMA is fun!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import string\n",
    "\n",
    "# --- Step 1: Load Data ---\n",
    "train_df = pd.read_csv(\"./../data/train.csv\")\n",
    "test_df  = pd.read_csv(\"./../data/test.csv\")\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(train_df.head())\n",
    "\n",
    "# --- Step 2: Character Tokenizer ---\n",
    "# Printable ASCII characters 32‚Äì126\n",
    "ascii_chars = [chr(i) for i in range(32, 127)]\n",
    "special_tokens = [\"[PAD]\", \"[SOS]\", \"[EOS]\"]\n",
    "\n",
    "itos = special_tokens + ascii_chars   # id ‚Üí char\n",
    "stoi = {ch: i for i, ch in enumerate(itos)}  # char ‚Üí id\n",
    "\n",
    "PAD_IDX = stoi[\"[PAD]\"]\n",
    "SOS_IDX = stoi[\"[SOS]\"]\n",
    "EOS_IDX = stoi[\"[EOS]\"]\n",
    "\n",
    "vocab_size = len(itos)\n",
    "\n",
    "print(\"\\nVocab size:\", vocab_size)\n",
    "print(\"PAD idx:\", PAD_IDX, \"SOS idx:\", SOS_IDX, \"EOS idx:\", EOS_IDX)\n",
    "\n",
    "# --- Encode / Decode functions ---\n",
    "def encode_text(text, add_special=True):\n",
    "    ids = [stoi[ch] for ch in text if ch in stoi]\n",
    "    if add_special:\n",
    "        ids = [SOS_IDX] + ids + [EOS_IDX]\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "def decode_ids(ids):\n",
    "    chars = []\n",
    "    for i in ids:\n",
    "        if i == PAD_IDX or i == SOS_IDX or i == EOS_IDX:\n",
    "            continue\n",
    "        chars.append(itos[i])\n",
    "    return \"\".join(chars)\n",
    "\n",
    "# --- Quick test ---\n",
    "sample = \"LMA is fun!\"\n",
    "encoded = encode_text(sample)\n",
    "decoded = decode_ids(encoded.tolist())\n",
    "\n",
    "print(\"Sample:\", sample)\n",
    "print(\"Encoded:\", encoded.tolist())\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7905ed",
   "metadata": {},
   "source": [
    "## Dataset + Collate\n",
    "\n",
    "- CharDataset ‚Üí loads strings, encodes them into token IDs.\n",
    "- collate_fn ‚Üí pads sequences per batch + stores lengths.\n",
    "- DataLoader ‚Üí provides batches for training & testing.\n",
    "- Prints shapes and a decoded sample to check correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb46380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_padded shape: torch.Size([32, 125])\n",
      "tgt_padded shape: torch.Size([32, 125])\n",
      "\n",
      "src_lengths: tensor([ 61, 117, 111,  71,  39])\n",
      "tgt_lengths: tensor([ 61, 117, 111,  71,  39])\n",
      "\n",
      "Example decoded input: cover kc we're extremists category stealth voter regulators\n",
      "Example decoded target: srotaluger retov htlaets yrogetac stsimertxe er'ew ck revoc\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df[\"input\"].tolist()\n",
    "        self.targets = df[\"target\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.inputs[idx]\n",
    "        tgt_text = self.targets[idx]\n",
    "\n",
    "        src_ids = encode_text(src_text, add_special=True)  # [SOS] ... [EOS]\n",
    "        tgt_ids = encode_text(tgt_text, add_special=True)\n",
    "\n",
    "        return src_ids, tgt_ids\n",
    "\n",
    "\n",
    "# --- Collate Function ---\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "\n",
    "    # Pad sequences\n",
    "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n",
    "    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_IDX)\n",
    "\n",
    "    # Lengths (before padding)\n",
    "    src_lengths = torch.tensor([len(x) for x in src_batch], dtype=torch.long)\n",
    "    tgt_lengths = torch.tensor([len(x) for x in tgt_batch], dtype=torch.long)\n",
    "\n",
    "    return src_padded, src_lengths, tgt_padded, tgt_lengths\n",
    "\n",
    "\n",
    "# --- Create Dataset + DataLoader ---\n",
    "train_ds = CharDataset(train_df)\n",
    "test_ds = CharDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# --- Quick Check ---\n",
    "src_padded, src_lengths, tgt_padded, tgt_lengths = next(iter(train_loader))\n",
    "\n",
    "print(\"src_padded shape:\", src_padded.shape)\n",
    "print(\"tgt_padded shape:\", tgt_padded.shape)\n",
    "print(\"\\nsrc_lengths:\", src_lengths[:5])\n",
    "print(\"tgt_lengths:\", tgt_lengths[:5])\n",
    "print(\"\\nExample decoded input:\", decode_ids(src_padded[0].tolist()))\n",
    "print(\"Example decoded target:\", decode_ids(tgt_padded[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f9f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 98\n"
     ]
    }
   ],
   "source": [
    "# --- Character Vocabulary (printable ASCII + special tokens) ---\n",
    "import string\n",
    "\n",
    "PAD_IDX = 0\n",
    "SOS_IDX = 1\n",
    "EOS_IDX = 2\n",
    "\n",
    "# Printable ASCII characters (32-126)\n",
    "chars = [chr(i) for i in range(32, 127)]\n",
    "VOCAB = {c: i+3 for i, c in enumerate(chars)}  # reserve 0,1,2 for PAD, SOS, EOS\n",
    "INV_VOCAB = {i: c for c, i in VOCAB.items()}\n",
    "\n",
    "VOCAB_SIZE = len(VOCAB) + 3  # include PAD, SOS, EOS\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09205993",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "- Embedding layer for characters\n",
    "- Positional encoding\n",
    "- Transformer encoder‚Äìdecoder (2 layers each)\n",
    "- Linear projection to vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beadd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- Positional Encoding (sinusoidal) ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, L, D]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- Baseline Transformer Model ---\n",
    "class CharTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, nhead=4, num_layers=2, dim_ff=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Embeddings\n",
    "        self.src_embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_IDX)\n",
    "        self.tgt_embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_IDX)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                    dim_feedforward=dim_ff, dropout=dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                    dim_feedforward=dim_ff, dropout=dropout, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output projection\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt_inp):\n",
    "        \"\"\"\n",
    "        src: [B, Ls]\n",
    "        tgt_inp: [B, Lt]\n",
    "        \"\"\"\n",
    "        # Embedding + positional encoding\n",
    "        src_emb = self.pos_encoder(self.src_embed(src))  # [B, Ls, D]\n",
    "        tgt_emb = self.pos_encoder(self.tgt_embed(tgt_inp))  # [B, Lt, D]\n",
    "\n",
    "        # Masks\n",
    "        src_key_padding_mask = (src == PAD_IDX)  # [B, Ls]\n",
    "        tgt_key_padding_mask = (tgt_inp == PAD_IDX)  # [B, Lt]\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(tgt_inp.size(1)).to(src.device)\n",
    "\n",
    "        # Encode\n",
    "        memory = self.encoder(src_emb, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        # Decode\n",
    "        out = self.decoder(tgt=tgt_emb, memory=memory,\n",
    "                            tgt_mask=causal_mask,\n",
    "                            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                            memory_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        # Project to vocab\n",
    "        logits = self.fc_out(out)  # [B, Lt, vocab_size]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fc2965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([32, 125, 95])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/functional.py:6041: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "baseline_model = CharTransformer(vocab_size=len(VOCAB), d_model=64, nhead=4, num_layers=2).to(device)\n",
    "\n",
    "# Dummy batch\n",
    "src_padded, src_lengths, tgt_padded, tgt_lengths = next(iter(train_loader))\n",
    "\n",
    "# Teacher forcing: shift target\n",
    "tgt_inp = tgt_padded[:, :-1]\n",
    "tgt_out = tgt_padded[:, 1:]\n",
    "\n",
    "# Forward pass\n",
    "logits = baseline_model(src_padded.to(device), tgt_inp.to(device))\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)  # [B, Lt-1, vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adf9a6b",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- AdamW optimizer\n",
    "- CrossEntropyLoss with ignore_index=PAD_IDX\n",
    "- Gradient clipping\n",
    "- Checkpoint saving every few epochs\n",
    "- Optional resume from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5949982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# ----------------------------\n",
    "# Training loop for baseline Transformer\n",
    "# ----------------------------\n",
    "def train_baseline(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader=None,   # optional validation loader\n",
    "    num_epochs=1000,\n",
    "    lr=1e-3,\n",
    "    device=None,\n",
    "    save_every=5,\n",
    "    val_every=50,\n",
    "    resume_path=None\n",
    "):\n",
    "    # --- Pick device automatically if not provided ---\n",
    "    if device is None:\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "    print(f\"Training on: {device}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)  # ignore <PAD> tokens\n",
    "\n",
    "    os.makedirs(\"checkpoints_baseline\", exist_ok=True)\n",
    "\n",
    "    # --- Resume from checkpoint ---\n",
    "    start_epoch = 1\n",
    "    if resume_path and os.path.exists(resume_path):\n",
    "        print(f\"üîÑ Resuming from checkpoint: {resume_path}\")\n",
    "        checkpoint = torch.load(resume_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"‚úÖ Resumed from epoch {checkpoint['epoch']} (loss {checkpoint['loss']:.4f})\")\n",
    "\n",
    "    # --- Training loop ---\n",
    "    for epoch in range(start_epoch, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            src, src_lengths, tgt, tgt_lengths = batch\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            # Shift target for teacher forcing\n",
    "            tgt_inp = tgt[:, :-1]   # decoder input\n",
    "            tgt_out = tgt[:, 1:]    # expected output\n",
    "\n",
    "            logits = model(src, tgt_inp)  # [B, L, vocab_size]\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                tgt_out.reshape(-1)\n",
    "            )\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"üìò Epoch {epoch}/{num_epochs} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        if val_loader is not None and epoch % val_every == 0:\n",
    "            val_loss, val_acc = evaluate_baseline(model, val_loader, criterion, device)\n",
    "            print(f\"üìó Validation (epoch {epoch}) - Loss: {val_loss:.4f}, Token Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # --- Save checkpoint ---\n",
    "        if epoch % save_every == 0 or epoch == num_epochs:\n",
    "            ckpt_path = f\"checkpoints_baseline/char_transformer_epoch{epoch}.pt\"\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"loss\": avg_loss\n",
    "            }, ckpt_path)\n",
    "            print(f\"‚úÖ Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation / Validation\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate_baseline(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "\n",
    "    for batch in val_loader:\n",
    "        src, src_lengths, tgt, tgt_lengths = batch\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        tgt_inp = tgt[:, :-1]\n",
    "        tgt_out = tgt[:, 1:]\n",
    "\n",
    "        logits = model(src, tgt_inp)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            tgt_out.reshape(-1)\n",
    "        )\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Accuracy (ignore PAD)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        mask = tgt_out != PAD_IDX\n",
    "        correct_tokens += ((preds == tgt_out) & mask).sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = 100.0 * correct_tokens / total_tokens\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314938ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: mps\n",
      "üîÑ Resuming from checkpoint: checkpoints_baseline/char_transformer_epoch45.pt\n",
      "‚úÖ Resumed from epoch 45 (loss 0.2808)\n",
      "üìò Epoch 46/1000 - Train Loss: 0.2778\n",
      "üìò Epoch 47/1000 - Train Loss: 0.2735\n",
      "üìò Epoch 48/1000 - Train Loss: 0.2846\n",
      "üìò Epoch 49/1000 - Train Loss: 0.2716\n",
      "üìò Epoch 50/1000 - Train Loss: 0.2597\n",
      "‚úÖ Saved checkpoint: checkpoints_baseline/char_transformer_epoch50.pt\n",
      "üìò Epoch 51/1000 - Train Loss: 0.2416\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_baseline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbaseline_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcheckpoints_baseline/char_transformer_epoch45.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mtrain_baseline\u001b[39m\u001b[34m(model, train_loader, val_loader, num_epochs, lr, device, save_every, val_every, resume_path)\u001b[39m\n\u001b[32m     69\u001b[39m optimizer.zero_grad()\n\u001b[32m     70\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m optimizer.step()\n\u001b[32m     74\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:36\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:221\u001b[39m, in \u001b[36mclip_grad_norm_\u001b[39m\u001b[34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m    216\u001b[39m         warnings.warn(\n\u001b[32m    217\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`parameters` is an empty generator, no gradient clipping will occur.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    218\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    219\u001b[39m         )\n\u001b[32m    220\u001b[39m grads = [p.grad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m total_norm = \u001b[43m_get_total_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_if_nonfinite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:36\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:96\u001b[39m, in \u001b[36m_get_total_norm\u001b[39m\u001b[34m(tensors, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m     91\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     92\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mforeach=True was passed, but can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     93\u001b[39m         )\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     95\u001b[39m         norms.extend(\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m             [\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvector_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m device_tensors]\n\u001b[32m     97\u001b[39m         )\n\u001b[32m     99\u001b[39m total_norm = torch.linalg.vector_norm(\n\u001b[32m    100\u001b[39m     torch.stack([norm.to(first_device) \u001b[38;5;28;01mfor\u001b[39;00m norm \u001b[38;5;129;01min\u001b[39;00m norms]), norm_type\n\u001b[32m    101\u001b[39m )\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch.logical_or(total_norm.isnan(), total_norm.isinf()):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_baseline(\n",
    "    model=baseline_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=None,\n",
    "    num_epochs=1000,\n",
    "    lr=1e-3,\n",
    "    device=device,\n",
    "    save_every=5,\n",
    "    resume_path=\"checkpoints_baseline/char_transformer_epoch45.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66512e3b",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070eac2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vocab size: 91\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# --- Load data ---\n",
    "train_df = pd.read_csv(\"./../data/train.csv\")\n",
    "test_df = pd.read_csv(\"./../data/test.csv\")\n",
    "\n",
    "# --- Build character set from train + test ---\n",
    "all_text = \"\".join(train_df[\"input\"].tolist() + train_df[\"target\"].tolist() +\n",
    "                   test_df[\"input\"].tolist() + test_df[\"target\"].tolist())\n",
    "\n",
    "chars = sorted(list(set(all_text)))\n",
    "\n",
    "# --- Add special tokens ---\n",
    "PAD_IDX = 0\n",
    "SOS_IDX = 1\n",
    "EOS_IDX = 2\n",
    "\n",
    "CHAR2IDX = {c: i+3 for i, c in enumerate(chars)}\n",
    "IDX2CHAR = {i+3: c for i, c in enumerate(chars)}\n",
    "\n",
    "CHAR2IDX[\"<PAD>\"] = PAD_IDX\n",
    "CHAR2IDX[\"<SOS>\"] = SOS_IDX\n",
    "CHAR2IDX[\"<EOS>\"] = EOS_IDX\n",
    "IDX2CHAR[PAD_IDX] = \"<PAD>\"\n",
    "IDX2CHAR[SOS_IDX] = \"<SOS>\"\n",
    "IDX2CHAR[EOS_IDX] = \"<EOS>\"\n",
    "\n",
    "VOCAB_SIZE = len(CHAR2IDX)\n",
    "print(f\"‚úÖ Vocab size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be82c317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/functional.py:6041: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test Loss: 29.6431, Perplexity: 7478683107328.0000, Token Accuracy: 5.16%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(29.643077177616124, tensor(7.4787e+12), 5.159886322149284)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CharDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, char2idx):\n",
    "        self.inputs = df[\"input\"].tolist()\n",
    "        self.targets = df[\"target\"].tolist()\n",
    "        self.char2idx = char2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert characters to indices\n",
    "        src = [self.char2idx[c] for c in self.inputs[idx]]\n",
    "        tgt = [self.char2idx[c] for c in self.targets[idx]]\n",
    "        return torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)\n",
    "\n",
    "def char_collate_fn(batch, device=None):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_lengths = [len(s) for s in src_batch]\n",
    "    tgt_lengths = [len(t) for t in tgt_batch]\n",
    "\n",
    "    max_src_len = max(src_lengths)\n",
    "    max_tgt_len = max(tgt_lengths)\n",
    "\n",
    "    src_padded = torch.zeros(len(batch), max_src_len, dtype=torch.long)\n",
    "    tgt_padded = torch.zeros(len(batch), max_tgt_len, dtype=torch.long)\n",
    "\n",
    "    for i, (s, t) in enumerate(zip(src_batch, tgt_batch)):\n",
    "        src_padded[i, :len(s)] = s\n",
    "        tgt_padded[i, :len(t)] = t\n",
    "\n",
    "    if device is not None:\n",
    "        src_padded = src_padded.to(device)\n",
    "        tgt_padded = tgt_padded.to(device)\n",
    "\n",
    "    return src_padded, torch.tensor(src_lengths), tgt_padded, torch.tensor(tgt_lengths)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_baseline(model, test_loader, device=None):\n",
    "    \"\"\"\n",
    "    Evaluate baseline character-level transformer on test set.\n",
    "    Computes avg loss, perplexity, and token-level accuracy.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)  # ignore padding\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "\n",
    "    for batch in test_loader:\n",
    "        src, src_lengths, tgt, tgt_lengths = batch\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        tgt_inp = tgt[:, :-1]\n",
    "        tgt_out = tgt[:, 1:]\n",
    "\n",
    "        logits = model(src, tgt_inp)  # [B, Lt-1, vocab_size]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "        total_loss += loss.item() * tgt_out.numel()\n",
    "\n",
    "        # Compute accuracy\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        mask = tgt_out != PAD_IDX\n",
    "        correct_tokens += ((preds == tgt_out) & mask).sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    accuracy = 100.0 * correct_tokens / total_tokens\n",
    "\n",
    "    print(f\"‚úÖ Test Loss: {avg_loss:.4f}, Perplexity: {perplexity:.4f}, Token Accuracy: {accuracy:.2f}%\")\n",
    "    return avg_loss, perplexity, accuracy\n",
    "\n",
    "# Example usage:\n",
    "test_ds = CharDataset(pd.read_csv(\"./../data/test.csv\"), char2idx=CHAR2IDX)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: char_collate_fn(b, device=device)\n",
    ")\n",
    "\n",
    "validate_baseline(baseline_model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3340b2c",
   "metadata": {},
   "source": [
    "## Test set prediction\n",
    "\n",
    "1. encode_text() and decode_text() are the same functions you used for your baseline dataset.\n",
    "2. SOS_IDX and EOS_IDX should be your special token indices (start/end).\n",
    "3. This uses greedy decoding only. No beam search or sampling needed.\n",
    "4. You can now submit predictions_normal.csv for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33545510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  LMA is fun!\n",
      "Prediction: hsi gnik $\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/functional.py:6041: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Force CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load checkpoint on CPU\n",
    "checkpoint_path = \"checkpoints_baseline/char_transformer_epoch10.pt\"\n",
    "baseline_model.load_state_dict(torch.load(checkpoint_path, map_location=device)[\"model_state\"])\n",
    "baseline_model.to(device)\n",
    "baseline_model.eval()\n",
    "\n",
    "# Vocabulary\n",
    "vocab = {c:i+3 for i,c in enumerate([chr(i) for i in range(32,127)])}\n",
    "inv_vocab = {i:c for c,i in vocab.items()}\n",
    "PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2\n",
    "\n",
    "def encode_text(text):\n",
    "    return [SOS_IDX] + [vocab[c] for c in text] + [EOS_IDX]\n",
    "\n",
    "def decode_text(ids):\n",
    "    return \"\".join([inv_vocab[i] for i in ids if i > 2])\n",
    "\n",
    "# Autoregressive prediction\n",
    "def generate_single_char_model(text, model, max_len=128):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor([encode_text(text)], dtype=torch.long, device=device)\n",
    "        output_ids = [SOS_IDX]\n",
    "\n",
    "        for t in range(max_len):\n",
    "            tgt_ids = torch.tensor([output_ids], dtype=torch.long, device=device)\n",
    "            logits = model(input_ids, tgt_ids)\n",
    "            next_id = logits[0, -1].argmax().item()\n",
    "            output_ids.append(next_id)\n",
    "            if next_id == EOS_IDX:\n",
    "                break\n",
    "\n",
    "        return decode_text(output_ids[1:])\n",
    "\n",
    "# Example\n",
    "sample_text = \"LMA is fun!\"\n",
    "prediction = generate_single_char_model(sample_text, baseline_model)\n",
    "print(\"Input: \", sample_text)\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da5ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ----------------------------\n",
    "# --- Load test data ---\n",
    "# ----------------------------\n",
    "test_df = pd.read_csv(\"./../data/test.csv\")\n",
    "\n",
    "class CharTestDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df[\"input\"].tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx]\n",
    "\n",
    "test_ds = CharTestDataset(test_df)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# ----------------------------\n",
    "# --- Character encoding / decoding ---\n",
    "# ----------------------------\n",
    "# Make sure these match your training vocab\n",
    "VOCAB = [PAD, SOS, EOS] + [chr(i) for i in range(32, 127)]\n",
    "PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2\n",
    "\n",
    "char2idx = {c: i for i, c in enumerate(VOCAB)}\n",
    "idx2char = {i: c for i, c in enumerate(VOCAB)}\n",
    "\n",
    "def encode_text(text):\n",
    "    return [SOS_IDX] + [char2idx.get(c, 3) for c in text] + [EOS_IDX]  # unknown=3\n",
    "\n",
    "def decode_text(indices):\n",
    "    # remove SOS/EOS/PAD\n",
    "    chars = [idx2char.get(i, \"\") for i in indices if i not in (PAD_IDX, SOS_IDX, EOS_IDX)]\n",
    "    return \"\".join(chars)\n",
    "\n",
    "# ----------------------------\n",
    "# --- Single prediction ---\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def generate_single_char_model(text, model, max_len=256):\n",
    "    device = torch.device(\"cpu\")  # force CPU for safety\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    input_ids = torch.tensor([encode_text(text)], dtype=torch.long, device=device)\n",
    "\n",
    "    # Start output with SOS\n",
    "    output_ids = [SOS_IDX]\n",
    "\n",
    "    for t in range(max_len):\n",
    "        tgt_ids = torch.tensor([output_ids], dtype=torch.long, device=device)\n",
    "        logits = model(input_ids, tgt_ids)  # returns [1, seq_len, vocab_size]\n",
    "        next_id = logits[0, -1].argmax().item()\n",
    "        output_ids.append(next_id)\n",
    "\n",
    "        if next_id == EOS_IDX:\n",
    "            break\n",
    "\n",
    "    return decode_text(output_ids)\n",
    "\n",
    "# ----------------------------\n",
    "# --- Generate predictions for all test samples ---\n",
    "# ----------------------------\n",
    "predictions = []\n",
    "for input_text in test_loader:\n",
    "    input_str = input_text[0]  # batch_size=1\n",
    "    pred = generate_single_char_model(input_str, baseline_model)\n",
    "    predictions.append(pred)\n",
    "\n",
    "# ----------------------------\n",
    "# --- Save CSV ---\n",
    "# ----------------------------\n",
    "test_df[\"prediction\"] = predictions\n",
    "os.makedirs(\"prediction\", exist_ok=True)\n",
    "output_path = \"./prediction/predictions_baseline.csv\"\n",
    "test_df.to_csv(output_path, index=False)\n",
    "print(f\"‚úÖ Predictions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c228213",
   "metadata": {},
   "source": [
    "# Evaluation & Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18880acd",
   "metadata": {},
   "source": [
    "##  Metrics evaluation script for both BLT and baseline models\n",
    "\n",
    "1.\tToken-level accuracy\n",
    "2.\tAverage sequence length (input tokens vs predicted tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63fdda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_predictions(pred_csv_path, original_csv_path):\n",
    "    # Load predictions\n",
    "    df_pred = pd.read_csv(pred_csv_path)\n",
    "    df_orig = pd.read_csv(original_csv_path)\n",
    "\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "    total_input_len = 0\n",
    "    total_output_len = 0\n",
    "\n",
    "    for idx in range(len(df_pred)):\n",
    "        pred = str(df_pred.loc[idx, \"prediction\"])\n",
    "        target = str(df_orig.loc[idx, \"target\"])\n",
    "        input_str = str(df_orig.loc[idx, \"input\"])\n",
    "\n",
    "        # Token-level comparison\n",
    "        min_len = min(len(pred), len(target))\n",
    "        correct_tokens += sum([pred[i] == target[i] for i in range(min_len)])\n",
    "        total_tokens += len(target)\n",
    "\n",
    "        # Sequence length stats\n",
    "        total_input_len += len(input_str)\n",
    "        total_output_len += len(pred)\n",
    "\n",
    "    token_accuracy = 100.0 * correct_tokens / total_tokens\n",
    "    avg_input_len = total_input_len / len(df_pred)\n",
    "    avg_output_len = total_output_len / len(df_pred)\n",
    "\n",
    "    print(f\"‚úÖ Evaluation for {pred_csv_path}\")\n",
    "    print(f\"Token-level Accuracy: {token_accuracy:.2f}%\")\n",
    "    print(f\"Average input length: {avg_input_len:.2f} chars\")\n",
    "    print(f\"Average predicted length: {avg_output_len:.2f} chars\")\n",
    "    print(\"-\" * 50)\n",
    "    return token_accuracy, avg_input_len, avg_output_len\n",
    "\n",
    "\n",
    "# --- Evaluate BLT predictions ---\n",
    "blt_acc, blt_in_len, blt_out_len = evaluate_predictions(\n",
    "    \"./prediction/blt_predictions.csv\", \"./../data/test.csv\"\n",
    ")\n",
    "\n",
    "# --- Evaluate Baseline predictions ---\n",
    "baseline_acc, base_in_len, base_out_len = evaluate_predictions(\n",
    "    \"./prediction/predictions_baseline.csv\", \"./../data/test.csv\"\n",
    ")\n",
    "\n",
    "# --- Comparison Summary ---\n",
    "print(\"üìä Comparison Summary:\")\n",
    "print(f\"BLT      | Acc: {blt_acc:.2f}%, Avg Input: {blt_in_len:.2f}, Avg Pred: {blt_out_len:.2f}\")\n",
    "print(f\"Baseline | Acc: {baseline_acc:.2f}%, Avg Input: {base_in_len:.2f}, Avg Pred: {base_out_len:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c6da7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
